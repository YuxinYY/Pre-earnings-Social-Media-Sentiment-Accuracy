{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1b7bbeea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import zstandard as zstd\n",
    "import json\n",
    "import pandas as pd\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4dad9279",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_zst_to_dataframe(file_path, max_records=None):\n",
    "    \"\"\"\n",
    "    Read a .zst compressed ndjson file and convert to pandas DataFrame\n",
    "    \n",
    "    Args:\n",
    "        file_path: Path to the .zst file\n",
    "        max_records: Optional limit on number of records to read (useful for testing)\n",
    "    \n",
    "    Returns:\n",
    "        pandas DataFrame with the Reddit data\n",
    "    \"\"\"\n",
    "    records = []\n",
    "    \n",
    "    with open(file_path, 'rb') as file_handle:\n",
    "        reader = zstd.ZstdDecompressor(max_window_size=2**31).stream_reader(file_handle)\n",
    "        buffer = ''\n",
    "        \n",
    "        while True:\n",
    "            # Read chunk\n",
    "            chunk = reader.read(2**20)  # 1MB chunks\n",
    "            if not chunk:\n",
    "                break\n",
    "                \n",
    "            # Decode and split into lines\n",
    "            decoded_chunk = chunk.decode('utf-8')\n",
    "            lines = (buffer + decoded_chunk).split('\\n')\n",
    "            \n",
    "            # Process all complete lines (all except the last one)\n",
    "            for line in lines[:-1]:\n",
    "                if line.strip():  # Skip empty lines\n",
    "                    try:\n",
    "                        record = json.loads(line)\n",
    "                        records.append(record)\n",
    "                        \n",
    "                        # Stop if we've hit the max_records limit\n",
    "                        if max_records and len(records) >= max_records:\n",
    "                            reader.close()\n",
    "                            return pd.DataFrame(records)\n",
    "                            \n",
    "                    except json.JSONDecodeError:\n",
    "                        # Skip malformed JSON lines\n",
    "                        continue\n",
    "            \n",
    "            # Keep the incomplete line in buffer\n",
    "            buffer = lines[-1]\n",
    "            \n",
    "        reader.close()\n",
    "    \n",
    "    return pd.DataFrame(records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8f762125",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_all_zst_files(directory_path, file_pattern=\"RS_*.zst\", max_records_per_file=None):\n",
    "    \"\"\"\n",
    "    Read multiple .zst files and combine into one DataFrame\n",
    "    \n",
    "    Args:\n",
    "        directory_path: Path to directory containing .zst files\n",
    "        file_pattern: Pattern to match files (default: \"RS_*.zst\")\n",
    "        max_records_per_file: Optional limit per file\n",
    "    \n",
    "    Returns:\n",
    "        Combined pandas DataFrame\n",
    "    \"\"\"\n",
    "    directory = Path(directory_path)\n",
    "    zst_files = list(directory.glob(file_pattern))\n",
    "    \n",
    "    all_dataframes = []\n",
    "    \n",
    "    for file_path in sorted(zst_files):\n",
    "        print(f\"Reading {file_path.name}...\")\n",
    "        df = read_zst_to_dataframe(file_path, max_records_per_file)\n",
    "        print(f\"  Loaded {len(df)} records\")\n",
    "        all_dataframes.append(df)\n",
    "    \n",
    "    # Combine all dataframes\n",
    "    if all_dataframes:\n",
    "        combined_df = pd.concat(all_dataframes, ignore_index=True)\n",
    "        print(f\"\\nTotal combined records: {len(combined_df)}\")\n",
    "        return combined_df\n",
    "    else:\n",
    "        print(\"No files found!\")\n",
    "        return pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "103ae0ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample data shape: (1000, 89)\n",
      "\n",
      "Columns available:\n",
      "['all_awardings', 'allow_live_comments', 'archived', 'author', 'author_created_utc', 'author_flair_background_color', 'author_flair_css_class', 'author_flair_richtext', 'author_flair_template_id', 'author_flair_text', 'author_flair_text_color', 'author_flair_type', 'author_fullname', 'author_patreon_flair', 'author_premium', 'can_gild', 'category', 'content_categories', 'contest_mode', 'created_utc', 'discussion_type', 'distinguished', 'domain', 'edited', 'gilded', 'gildings', 'hidden', 'hide_score', 'id', 'is_created_from_ads_ui', 'is_crosspostable', 'is_meta', 'is_original_content', 'is_reddit_media_domain', 'is_robot_indexable', 'is_self', 'is_video', 'link_flair_background_color', 'link_flair_css_class', 'link_flair_richtext', 'link_flair_template_id', 'link_flair_text', 'link_flair_text_color', 'link_flair_type', 'locked', 'media', 'media_embed', 'media_only', 'name', 'no_follow', 'num_comments', 'num_crossposts', 'over_18', 'parent_whitelist_status', 'permalink', 'pinned', 'pwls', 'quarantine', 'removed_by_category', 'retrieved_utc', 'score', 'secure_media', 'secure_media_embed', 'selftext', 'send_replies', 'spoiler', 'stickied', 'subreddit', 'subreddit_id', 'subreddit_subscribers', 'subreddit_type', 'suggested_sort', 'thumbnail', 'thumbnail_height', 'thumbnail_width', 'title', 'top_awarded_type', 'total_awards_received', 'treatment_tags', 'upvote_ratio', 'url', 'whitelist_status', 'wls', 'url_overridden_by_dest', 'media_metadata', 'gallery_data', 'is_gallery', 'post_hint', 'preview']\n",
      "\n",
      "First few records:\n",
      "  all_awardings  allow_live_comments  archived              author  \\\n",
      "0            []                False     False             Bootso_   \n",
      "1            []                False     False      Carneythegamer   \n",
      "2            []                False     False             Bootso_   \n",
      "3            []                False     False  JollyIntention9213   \n",
      "4            []                False     False           [deleted]   \n",
      "\n",
      "   author_created_utc author_flair_background_color author_flair_css_class  \\\n",
      "0        1.620071e+09                          None                   None   \n",
      "1        1.622506e+09                          None                   None   \n",
      "2        1.620071e+09                          None                   None   \n",
      "3        1.611865e+09                          None                   None   \n",
      "4                 NaN                                                 None   \n",
      "\n",
      "  author_flair_richtext author_flair_template_id author_flair_text  ...  \\\n",
      "0                    []                     None              None  ...   \n",
      "1                    []                     None              None  ...   \n",
      "2                    []                     None              None  ...   \n",
      "3                    []                     None              None  ...   \n",
      "4                   NaN                     None              None  ...   \n",
      "\n",
      "  upvote_ratio                                                url  \\\n",
      "0          1.0  https://www.reddit.com/r/wallstreetbets/commen...   \n",
      "1          1.0  https://www.reddit.com/r/wallstreetbets/commen...   \n",
      "2          1.0  https://www.reddit.com/r/wallstreetbets/commen...   \n",
      "3          1.0  https://www.reddit.com/r/wallstreetbets/commen...   \n",
      "4          1.0  https://www.reddit.com/r/wallstreetbets/commen...   \n",
      "\n",
      "  whitelist_status wls url_overridden_by_dest  media_metadata gallery_data  \\\n",
      "0         some_ads   7                    NaN             NaN          NaN   \n",
      "1         some_ads   7                    NaN             NaN          NaN   \n",
      "2         some_ads   7                    NaN             NaN          NaN   \n",
      "3         some_ads   7                    NaN             NaN          NaN   \n",
      "4         some_ads   7                    NaN             NaN          NaN   \n",
      "\n",
      "  is_gallery  post_hint  preview  \n",
      "0        NaN        NaN      NaN  \n",
      "1        NaN        NaN      NaN  \n",
      "2        NaN        NaN      NaN  \n",
      "3        NaN        NaN      NaN  \n",
      "4        NaN        NaN      NaN  \n",
      "\n",
      "[5 rows x 89 columns]\n"
     ]
    }
   ],
   "source": [
    "# Example usage:\n",
    "\n",
    "# 1. Read a single file (test with first 1000 records)\n",
    "file_path = r\"F:\\reddit data\\reddit\\submissions\\filtered data\\wallstreetbets\\RS_2021-06.zst\"\n",
    "df_sample = read_zst_to_dataframe(file_path, max_records=1000)\n",
    "\n",
    "print(\"Sample data shape:\", df_sample.shape)\n",
    "print(\"\\nColumns available:\")\n",
    "print(df_sample.columns.tolist())\n",
    "\n",
    "print(\"\\nFirst few records:\")\n",
    "print(df_sample.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "651c74db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading RS_2021-06.zst...\n",
      "  Loaded 94987 records\n",
      "Reading RS_2021-07.zst...\n",
      "  Loaded 31560 records\n",
      "Reading RS_2021-08.zst...\n",
      "  Loaded 25759 records\n",
      "\n",
      "Total combined records: 152306\n",
      "Full dataset shape: (152306, 99)\n",
      "Memory usage: 494.19725799560547 MB\n"
     ]
    }
   ],
   "source": [
    "# 2. Read all files (this might be a lot of data!)\n",
    "directory_path = r\"F:\\reddit data\\reddit\\submissions\\filtered data\\wallstreetbets\"\n",
    "df_all = read_all_zst_files(directory_path)\n",
    "\n",
    "print(\"Full dataset shape:\", df_all.shape)\n",
    "print(\"Memory usage:\", df_all.memory_usage(deep=True).sum() / 1024**2, \"MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c624c49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 3. For very large datasets, you might want to process in chunks\n",
    "# def process_zst_in_chunks(file_path, chunk_size=10000):\n",
    "#     \"\"\"\n",
    "#     Process large .zst files in chunks to avoid memory issues\n",
    "#     \"\"\"\n",
    "#     chunk_count = 0\n",
    "    \n",
    "#     with open(file_path, 'rb') as file_handle:\n",
    "#         reader = zstd.ZstdDecompressor(max_window_size=2**31).stream_reader(file_handle)\n",
    "#         buffer = ''\n",
    "#         records = []\n",
    "        \n",
    "#         while True:\n",
    "#             chunk = reader.read(2**20)\n",
    "#             if not chunk:\n",
    "#                 break\n",
    "                \n",
    "#             decoded_chunk = chunk.decode('utf-8')\n",
    "#             lines = (buffer + decoded_chunk).split('\\n')\n",
    "            \n",
    "#             for line in lines[:-1]:\n",
    "#                 if line.strip():\n",
    "#                     try:\n",
    "#                         record = json.loads(line)\n",
    "#                         records.append(record)\n",
    "                        \n",
    "#                         if len(records) >= chunk_size:\n",
    "#                             # Process this chunk\n",
    "#                             df_chunk = pd.DataFrame(records)\n",
    "#                             chunk_count += 1\n",
    "#                             print(f\"Processing chunk {chunk_count} with {len(df_chunk)} records\")\n",
    "                            \n",
    "#                             # Do your analysis on df_chunk here\n",
    "#                             # Example: print(df_chunk['title'].head())\n",
    "                            \n",
    "#                             records = []  # Reset for next chunk\n",
    "                            \n",
    "#                     except json.JSONDecodeError:\n",
    "#                         continue\n",
    "            \n",
    "#             buffer = lines[-1]\n",
    "        \n",
    "#         # Process remaining records\n",
    "#         if records:\n",
    "#             df_chunk = pd.DataFrame(records)\n",
    "#             chunk_count += 1\n",
    "#             print(f\"Processing final chunk {chunk_count} with {len(df_chunk)} records\")\n",
    "        \n",
    "#         reader.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5d376048",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load data with only useful fields\n",
    "# def read_zst_filtered_fields(file_path, fields_to_keep, max_records=None):\n",
    "#     records = []\n",
    "    \n",
    "#     with open(file_path, 'rb') as file_handle:\n",
    "#         reader = zstd.ZstdDecompressor(max_window_size=2**31).stream_reader(file_handle)\n",
    "#         buffer = ''\n",
    "        \n",
    "#         while True:\n",
    "#             chunk = reader.read(2**20)\n",
    "#             if not chunk:\n",
    "#                 break\n",
    "                \n",
    "#             decoded_chunk = chunk.decode('utf-8')\n",
    "#             lines = (buffer + decoded_chunk).split('\\n')\n",
    "            \n",
    "#             for line in lines[:-1]:\n",
    "#                 if line.strip():\n",
    "#                     try:\n",
    "#                         full_record = json.loads(line)\n",
    "#                         # Only keep specified fields\n",
    "#                         filtered_record = {field: full_record.get(field) for field in fields_to_keep if field in full_record}\n",
    "#                         records.append(filtered_record)\n",
    "                        \n",
    "#                         if max_records and len(records) >= max_records:\n",
    "#                             reader.close()\n",
    "#                             return pd.DataFrame(records)\n",
    "                            \n",
    "#                     except json.JSONDecodeError:\n",
    "#                         continue\n",
    "            \n",
    "#             buffer = lines[-1]\n",
    "            \n",
    "#         reader.close()\n",
    "    \n",
    "#     return pd.DataFrame(records)\n",
    "\n",
    "def read_zst_filtered_fields(file_path, fields_to_keep, chunk_size=1024*1024):\n",
    "    \"\"\"\n",
    "    Read a .zst file and return a DataFrame with only specified fields\n",
    "    \"\"\"\n",
    "    records = []\n",
    "    \n",
    "    with open(file_path, 'rb') as f:\n",
    "        dctx = zstd.ZstdDecompressor()\n",
    "        stream_reader = dctx.stream_reader(f)\n",
    "        \n",
    "        buffer = \"\"\n",
    "        \n",
    "        while True:\n",
    "            chunk = stream_reader.read(chunk_size)\n",
    "            if not chunk:\n",
    "                break\n",
    "            \n",
    "            try:\n",
    "                decoded_chunk = chunk.decode('utf-8')\n",
    "            except UnicodeDecodeError:\n",
    "                # Handle encoding errors gracefully\n",
    "                decoded_chunk = chunk.decode('utf-8', errors='ignore')\n",
    "                print(f\"  Warning: Encountered encoding issues in {file_path.name}, some characters may be missing\")\n",
    "            \n",
    "            lines = (buffer + decoded_chunk).split('\\n')\n",
    "            \n",
    "            # Keep the last incomplete line in buffer\n",
    "            buffer = lines[-1]\n",
    "            \n",
    "            for line in lines[:-1]:\n",
    "                if line.strip():\n",
    "                    try:\n",
    "                        data = json.loads(line)\n",
    "                        # Filter to keep only specified fields\n",
    "                        filtered_data = {field: data.get(field) for field in fields_to_keep if field in data}\n",
    "                        if filtered_data:  # Only add if we have some data\n",
    "                            records.append(filtered_data)\n",
    "                    except json.JSONDecodeError:\n",
    "                        continue  # Skip malformed JSON lines\n",
    "        \n",
    "        # Process any remaining data in buffer\n",
    "        if buffer.strip():\n",
    "            try:\n",
    "                data = json.loads(buffer)\n",
    "                filtered_data = {field: data.get(field) for field in fields_to_keep if field in data}\n",
    "                if filtered_data:\n",
    "                    records.append(filtered_data)\n",
    "            except json.JSONDecodeError:\n",
    "                pass\n",
    "    \n",
    "    return pd.DataFrame(records)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2a9ae018",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def read_all_files_filtered(directory_path, fields_to_keep, file_pattern=\"*.zst\"):\n",
    "#     \"\"\"\n",
    "#     Read all .zst files with only specified fields\n",
    "#     \"\"\"\n",
    "#     from pathlib import Path\n",
    "    \n",
    "#     directory = Path(directory_path)\n",
    "#     zst_files = list(directory.glob(file_pattern))\n",
    "    \n",
    "#     all_dataframes = []\n",
    "    \n",
    "#     for file_path in sorted(zst_files):\n",
    "#         print(f\"Reading {file_path.name} (filtered fields)...\")\n",
    "#         df = read_zst_filtered_fields(file_path, fields_to_keep)\n",
    "#         print(f\"  Loaded {len(df)} records with {len(df.columns)} columns\")\n",
    "#         all_dataframes.append(df)\n",
    "    \n",
    "#     if all_dataframes:\n",
    "#         combined_df = pd.concat(all_dataframes, ignore_index=True)\n",
    "#         print(f\"\\nTotal combined records: {len(combined_df)}\")\n",
    "#         print(f\"Memory usage: {combined_df.memory_usage(deep=True).sum() / 1024**2:.1f} MB\")\n",
    "#         return combined_df\n",
    "#     else:\n",
    "#         return pd.DataFrame()\n",
    "\n",
    "def read_all_files_filtered(directory_path, fields_to_keep, file_pattern=\"*.zst\"):\n",
    "    \"\"\"\n",
    "    Read all .zst files with only specified fields\n",
    "    \"\"\"\n",
    "    from pathlib import Path\n",
    "    \n",
    "    directory = Path(directory_path)\n",
    "    zst_files = list(directory.glob(file_pattern))\n",
    "    \n",
    "    all_dataframes = []\n",
    "    \n",
    "    for file_path in sorted(zst_files):\n",
    "        print(f\"Reading {file_path.name} (filtered fields)...\")\n",
    "        try:\n",
    "            df = read_zst_filtered_fields(file_path, fields_to_keep)\n",
    "            print(f\"  Loaded {len(df)} records with {len(df.columns)} columns\")\n",
    "            all_dataframes.append(df)\n",
    "        except Exception as e:\n",
    "            print(f\"  Error reading {file_path.name}: {e}\")\n",
    "            print(f\"  Skipping this file...\")\n",
    "            continue\n",
    "    \n",
    "    if all_dataframes:\n",
    "        combined_df = pd.concat(all_dataframes, ignore_index=True)\n",
    "        print(f\"\\nTotal combined records: {len(combined_df)}\")\n",
    "        print(f\"Memory usage: {combined_df.memory_usage(deep=True).sum() / 1024**2:.1f} MB\")\n",
    "        return combined_df\n",
    "    else:\n",
    "        return pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c4d801ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading RC_2021-06.zst (filtered fields)...\n",
      "  Loaded 2186516 records with 5 columns\n",
      "Reading RC_2021-07.zst (filtered fields)...\n",
      "  Warning: Encountered encoding issues in RC_2021-07.zst, some characters may be missing\n",
      "  Warning: Encountered encoding issues in RC_2021-07.zst, some characters may be missing\n",
      "  Warning: Encountered encoding issues in RC_2021-07.zst, some characters may be missing\n",
      "  Warning: Encountered encoding issues in RC_2021-07.zst, some characters may be missing\n",
      "  Warning: Encountered encoding issues in RC_2021-07.zst, some characters may be missing\n",
      "  Warning: Encountered encoding issues in RC_2021-07.zst, some characters may be missing\n",
      "  Loaded 1100198 records with 5 columns\n",
      "Reading RC_2021-08.zst (filtered fields)...\n",
      "  Loaded 1011270 records with 5 columns\n",
      "Reading RS_2021-06.zst (filtered fields)...\n",
      "  Loaded 94987 records with 10 columns\n",
      "Reading RS_2021-07.zst (filtered fields)...\n",
      "  Loaded 31560 records with 10 columns\n",
      "Reading RS_2021-08.zst (filtered fields)...\n",
      "  Loaded 25759 records with 10 columns\n",
      "\n",
      "Total combined records: 4450290\n",
      "Memory usage: 1460.1 MB\n"
     ]
    }
   ],
   "source": [
    "# Example: Load all data with only essential fields\n",
    "essential_fields = [\n",
    "    'id', 'title', 'selftext', 'author', 'created_utc', 'score', \n",
    "    'num_comments', 'url', 'subreddit', 'upvote_ratio'\n",
    "]\n",
    "\n",
    "\n",
    "directory_path = r\"F:\\reddit data\\reddit\\submissions\\filtered data\\wallstreetbets\"\n",
    "df_wallstreetbets = read_all_files_filtered(directory_path, essential_fields)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "71517a0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing data from one subreddit: r/wallstreetbets\n",
    "# df_wallstreetbets\n",
    "df_test = df_wallstreetbets[(df_wallstreetbets['selftext']!=\"[removed]\") & (df_wallstreetbets['selftext']!=\"[deleted]\") & (df_wallstreetbets['selftext'].isna()!=True)]\n",
    "df_test.to_csv(\"test.csv\",index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c6cbfeda",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>author</th>\n",
       "      <th>created_utc</th>\n",
       "      <th>score</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>title</th>\n",
       "      <th>selftext</th>\n",
       "      <th>num_comments</th>\n",
       "      <th>url</th>\n",
       "      <th>upvote_ratio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4297989</th>\n",
       "      <td>npgy6l</td>\n",
       "      <td>TheMarkIII</td>\n",
       "      <td>1622505919</td>\n",
       "      <td>1</td>\n",
       "      <td>wallstreetbets</td>\n",
       "      <td>Caught a glimpse of AMC on its way to the moon!!</td>\n",
       "      <td></td>\n",
       "      <td>1.0</td>\n",
       "      <td>https://i.redd.it/tuw6d5qlpj271.jpg</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4297991</th>\n",
       "      <td>nph22g</td>\n",
       "      <td>macfern</td>\n",
       "      <td>1622506236</td>\n",
       "      <td>1</td>\n",
       "      <td>wallstreetbets</td>\n",
       "      <td>Wanda Group Fully Withdraws From AMC in Nearly...</td>\n",
       "      <td></td>\n",
       "      <td>1.0</td>\n",
       "      <td>https://www.google.com/amp/s/www.theepochtimes...</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4297993</th>\n",
       "      <td>nph34u</td>\n",
       "      <td>randyzmzzzz</td>\n",
       "      <td>1622506330</td>\n",
       "      <td>1</td>\n",
       "      <td>wallstreetbets</td>\n",
       "      <td>AMC at 42nd street, NYC</td>\n",
       "      <td></td>\n",
       "      <td>1.0</td>\n",
       "      <td>https://i.redd.it/9i2q5u6tqj271.jpg</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4297996</th>\n",
       "      <td>nph718</td>\n",
       "      <td>TheHighness1</td>\n",
       "      <td>1622506662</td>\n",
       "      <td>1</td>\n",
       "      <td>wallstreetbets</td>\n",
       "      <td>Ape Strong together</td>\n",
       "      <td></td>\n",
       "      <td>1.0</td>\n",
       "      <td>https://i.redd.it/fzn8646trj271.jpg</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4297997</th>\n",
       "      <td>nph8m3</td>\n",
       "      <td>wowwkwkssiw</td>\n",
       "      <td>1622506792</td>\n",
       "      <td>1</td>\n",
       "      <td>wallstreetbets</td>\n",
       "      <td>Elon Musk Charged With Second Degree Murder</td>\n",
       "      <td></td>\n",
       "      <td>1.0</td>\n",
       "      <td>https://theproffessionalbroker.blogspot.com/20...</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4450281</th>\n",
       "      <td>pfhn3d</td>\n",
       "      <td>_disguy</td>\n",
       "      <td>1630452959</td>\n",
       "      <td>2117</td>\n",
       "      <td>wallstreetbets</td>\n",
       "      <td>GME has consistently spiked every T-17 days be...</td>\n",
       "      <td></td>\n",
       "      <td>252.0</td>\n",
       "      <td>https://i.redd.it/zhjf7do34sk71.png</td>\n",
       "      <td>0.93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4450283</th>\n",
       "      <td>pfhpj2</td>\n",
       "      <td>Kobeblackmambattv</td>\n",
       "      <td>1630453193</td>\n",
       "      <td>75</td>\n",
       "      <td>wallstreetbets</td>\n",
       "      <td>$GENI DD | I'm either a $GENIus or a retard. 2...</td>\n",
       "      <td>&amp;amp;#x200B;\\n\\nhttps://preview.redd.it/bfclvu...</td>\n",
       "      <td>45.0</td>\n",
       "      <td>https://www.reddit.com/r/wallstreetbets/commen...</td>\n",
       "      <td>0.89</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4450284</th>\n",
       "      <td>pfhq3j</td>\n",
       "      <td>poohyawn</td>\n",
       "      <td>1630453246</td>\n",
       "      <td>2</td>\n",
       "      <td>wallstreetbets</td>\n",
       "      <td>Does anyone know what the first stock symbol i...</td>\n",
       "      <td></td>\n",
       "      <td>14.0</td>\n",
       "      <td>https://i.redd.it/4jomom135sk71.jpg</td>\n",
       "      <td>0.61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4450285</th>\n",
       "      <td>pfhtyf</td>\n",
       "      <td>outphase84</td>\n",
       "      <td>1630453627</td>\n",
       "      <td>2</td>\n",
       "      <td>wallstreetbets</td>\n",
       "      <td>1.4K to 7.1K overnight on FIVN puts. Thanks ZM!</td>\n",
       "      <td></td>\n",
       "      <td>2.0</td>\n",
       "      <td>https://i.redd.it/3drt4ft76sk71.jpg</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4450286</th>\n",
       "      <td>pfhw6s</td>\n",
       "      <td>outphase84</td>\n",
       "      <td>1630453851</td>\n",
       "      <td>82</td>\n",
       "      <td>wallstreetbets</td>\n",
       "      <td>1.4K to 7.K overnight on FIVN puts. Thanks ZM!</td>\n",
       "      <td></td>\n",
       "      <td>35.0</td>\n",
       "      <td>https://www.reddit.com/gallery/pfhw6s</td>\n",
       "      <td>0.90</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>50351 rows Ã— 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             id             author  created_utc  score       subreddit  \\\n",
       "4297989  npgy6l         TheMarkIII   1622505919      1  wallstreetbets   \n",
       "4297991  nph22g            macfern   1622506236      1  wallstreetbets   \n",
       "4297993  nph34u        randyzmzzzz   1622506330      1  wallstreetbets   \n",
       "4297996  nph718       TheHighness1   1622506662      1  wallstreetbets   \n",
       "4297997  nph8m3        wowwkwkssiw   1622506792      1  wallstreetbets   \n",
       "...         ...                ...          ...    ...             ...   \n",
       "4450281  pfhn3d            _disguy   1630452959   2117  wallstreetbets   \n",
       "4450283  pfhpj2  Kobeblackmambattv   1630453193     75  wallstreetbets   \n",
       "4450284  pfhq3j           poohyawn   1630453246      2  wallstreetbets   \n",
       "4450285  pfhtyf         outphase84   1630453627      2  wallstreetbets   \n",
       "4450286  pfhw6s         outphase84   1630453851     82  wallstreetbets   \n",
       "\n",
       "                                                     title  \\\n",
       "4297989   Caught a glimpse of AMC on its way to the moon!!   \n",
       "4297991  Wanda Group Fully Withdraws From AMC in Nearly...   \n",
       "4297993                            AMC at 42nd street, NYC   \n",
       "4297996                                Ape Strong together   \n",
       "4297997        Elon Musk Charged With Second Degree Murder   \n",
       "...                                                    ...   \n",
       "4450281  GME has consistently spiked every T-17 days be...   \n",
       "4450283  $GENI DD | I'm either a $GENIus or a retard. 2...   \n",
       "4450284  Does anyone know what the first stock symbol i...   \n",
       "4450285    1.4K to 7.1K overnight on FIVN puts. Thanks ZM!   \n",
       "4450286     1.4K to 7.K overnight on FIVN puts. Thanks ZM!   \n",
       "\n",
       "                                                  selftext  num_comments  \\\n",
       "4297989                                                              1.0   \n",
       "4297991                                                              1.0   \n",
       "4297993                                                              1.0   \n",
       "4297996                                                              1.0   \n",
       "4297997                                                              1.0   \n",
       "...                                                    ...           ...   \n",
       "4450281                                                            252.0   \n",
       "4450283  &amp;#x200B;\\n\\nhttps://preview.redd.it/bfclvu...          45.0   \n",
       "4450284                                                             14.0   \n",
       "4450285                                                              2.0   \n",
       "4450286                                                             35.0   \n",
       "\n",
       "                                                       url  upvote_ratio  \n",
       "4297989                https://i.redd.it/tuw6d5qlpj271.jpg          1.00  \n",
       "4297991  https://www.google.com/amp/s/www.theepochtimes...          1.00  \n",
       "4297993                https://i.redd.it/9i2q5u6tqj271.jpg          1.00  \n",
       "4297996                https://i.redd.it/fzn8646trj271.jpg          1.00  \n",
       "4297997  https://theproffessionalbroker.blogspot.com/20...          1.00  \n",
       "...                                                    ...           ...  \n",
       "4450281                https://i.redd.it/zhjf7do34sk71.png          0.93  \n",
       "4450283  https://www.reddit.com/r/wallstreetbets/commen...          0.89  \n",
       "4450284                https://i.redd.it/4jomom135sk71.jpg          0.61  \n",
       "4450285                https://i.redd.it/3drt4ft76sk71.jpg          1.00  \n",
       "4450286              https://www.reddit.com/gallery/pfhw6s          0.90  \n",
       "\n",
       "[50351 rows x 10 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df981bd9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82d6d465",
   "metadata": {},
   "outputs": [],
   "source": [
    "#storing data into disk\n",
    "dfs.to_csv(r\"F:\\reddit data\\reddit\\submissions\\filtered data\\filteredsubandcoms.csv\",index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
